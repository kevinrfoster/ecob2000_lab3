---
title: "Lab 3"
output: github_document
---

### Econ B2000, MA Econometrics
### Kevin R Foster, the Colin Powell School at the City College of New York, CUNY
### Fall 2024

For this lab, we will use simple k-nn techniques of machine learning to try to guess about whether people reported long Covid symptoms. K-nn is a fancy name for a really simple procedure:

* take an unclassified observation
* look for classified observations _near_ it
* guess that it is like its neighbors

We can understand the k-nn method without any statistics more complicated than means (of subgroups) and standard deviations. It's called "k-nn" since it uses the k Nearest Neighbors, where k is usually a small number, for instance the 3 nearest neighbors would be setting k=3.

We will compare this k-nn technique with simple conditional means and create some graphs to help show what's going on. Although, note, some things are just tough to predict so don't get too frustrated if your k-nn doesn't do well.

Split into groups. You can discuss, start coding and then chat at the end of class (as usual, then write up results in homework). 

The idea here is to try to classify whether people got long Covid. You probably have some thoughts about what is important in that classification, especially after last week's look at people's decisions about vaccination. Here we try to train a model, using the HPS data again.

Start with looking at the differences in means of some of the variables and put that together with your own knowledge of the world. We'll classify based on education and age. (Not making a statement of causation! Think about why causal arrow could go in either direction.) Some of your previous work should come in handy.

Then use a k-nn classification.


```{r eval = TRUE, echo=TRUE, warning=FALSE, message=FALSE}
setwd("..\\..\\data\\Household Pulse Survey Phase4 Cycle2")
load("Household_Pulse_data_ph4c2.RData")
setwd("..\\..\\EcoB2000_projects\\ecob2000_lab3")
# note your directory structures might be different so "setwd" would be different on your machine
require(tidyverse)
require(class)
require(caret)

# we will want to use these later
Household_Pulse_data$Age <- 2024 - Household_Pulse_data$TBIRTH_YEAR 
# remember topcoding, perhaps you can do a bit better to take account of that?

Household_Pulse_data$Educ_years <- fct_recode(Household_Pulse_data$EEDUC,
                                              "8" = "less than hs",
                                              "11" = "some hs",
                                              "12" = "HS diploma",
                                              "13" = "some coll",
                                              "14" = "assoc deg",
                                              "16" = "bach deg",
                                              "18" = "adv deg")

Household_Pulse_data$Educ_years <- as.numeric(levels(Household_Pulse_data$Educ_years))[Household_Pulse_data$Educ_years]


```

You can choose what to do with the NA values -- the data has long Covid conditional on someone answering yes to previous question about whether they had Covid. This crosstab shows that:

```{r eval = TRUE}
xtabs(formula = ~HADCOVIDRV + LONGCOVID, data = Household_Pulse_data)
```


Therefore restrict to those who had Covid:
```{r eval = TRUE}

dat_hadcovid <- Household_Pulse_data %>% filter(HADCOVIDRV == "yes tested + or doc told had Covid")

```

For now I'll leave in the NA responses but you might choose something else.

```{r eval=FALSE}
# always check to make sure this went right!
summary(dat_hadcovid)
```

Lots of interesting changes here.

What variables do you think are relevant in classifying whether somebody reported that they got long Covid? I'll try education and Age. You should find other data to try to do better.

Here is some code to get you started. 

```{r eval = TRUE}
norm_varb <- function(X_in) {
  (X_in - min(X_in, na.rm = TRUE))/( max(X_in, na.rm = TRUE) - min(X_in, na.rm = TRUE)  )
}

data_use_prelim <- data.frame(norm_varb(dat_hadcovid$Age),norm_varb(dat_hadcovid$Educ_years))

good_obs_data_use <- complete.cases(data_use_prelim,dat_hadcovid$LONGCOVID)
dat_use <- subset(data_use_prelim,good_obs_data_use)
y_use <- subset(dat_hadcovid$LONGCOVID,good_obs_data_use)
```

Note that knn doesn't like factors, it wants numbers as inputs. We often **normalize** the input variables and you should be able to convince yourself that a formula,
$$
(X - X_{min})/(X_{max} - X_{min})
$$
does indeed output only numbers between zero and one.

Next split the data into 2 parts: one part to train the algo, then the other part to test how well it works for new data. Here we use an 80/20 split so 80% of the data is used to train and 20% to check how well that training worked. You can try 70/30 or 60/40 or other values.
```{r eval = TRUE}
set.seed(12345)
NN_obs <- sum(good_obs_data_use == 1)
select1 <- (runif(NN_obs) < 0.8)
train_data <- subset(dat_use,select1)
test_data <- subset(dat_use,(!select1))
cl_data <- y_use[select1]
true_data <- y_use[!select1]
```


Finally run the k-nn algo and compare against the simple means,
```{r eval = FALSE}
summary(cl_data)
summary(train_data)

for (indx in seq(1, 9, by= 2)) {
 pred_y <- knn3Train(train_data, test_data, cl_data, k = indx, l = 0, prob = FALSE, use.all = TRUE)
 num_correct_labels <- sum(pred_y == true_data)
 correct_rate <- num_correct_labels/length(true_data)
 print(c(indx,correct_rate))
}
```

Now find some other data that will do a better job of classifying. How good can you get it to be? At what point do you think there might be a tradeoff between better classifying the training data and doing worse at classifying the test data? Can you show some graphs that help explain what's going on?


